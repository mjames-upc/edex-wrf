# ==============================================================================================
# THE UEMS COMPUTATIONAL ENVIRONMENT CONFIGURATION FILE
# ==============================================================================================
#
# SO WHAT DOES THIS CONFIGURATION FILE DO FOR ME?
#
#   This file contains the configuration settings used when running a simulation
#   on multiple processors and nodes. In you are running the UEMS on a cluster
#   then changes are you will need to edit this file. If you are running simulations
#   on a single system then you probably do not need to make any changes here; however,
#   it is recommended that you at least become familiar with the valuable information
#   provided below.
#
# RUNNING ON A CLUSTER? NO PROBLEM, THE UEMS HAS YOU COVERED!
#
#   The UEMS has everything you need to run your simulations across a network of
#   workstations. The package includes MPICH to handle all the communication along
#   with other tools to make your life easier. Nonetheless, the UEMS can't do everything
#   for you, as much as we try. If your plans are to run simulations on a local network
#   then you are responsible for the passwordless SSH configuration between each of
#   the machines in your cluster.  Additionally, the UEMS and its environment variables
#   must be exported to each of the machines.  Hey, the UEMS Overlords not only make
#   the rules, they enforce them too.
#
#   Fortunately, tools are available to assist you in this process. The "netcheck"
#   routine can be used to test the viability of your cluster once you have completed
#   all the dirty work (SSH configuration, exporting directories and setting up user
#   accounts. To run netcheck:
#
#       %  netcheck <system 1>  <system 2> <system 3> ... <system N>
#
#   Where you replace "<system #>" with each hostname to be included in your runs
#
#   Another information-packed utility is "sysinfo", which collects UEMS-related information
#   from each of the machines and prints it out to the screen.  Sure to be a hit at
#   parties.  To run sysinfo:
#
#       %  sysinfo <system 1>  <system 2> <system 3> ... <system N>
#
#   Hey look, it works just like the "netcheck" utility, only with different output.
#
#   LOG:  R.Rozumalski - NWS January 2016
#
# ==============================================================================================
# ==============================================================================================
#

#
#  OPTION:  [REAL|WRFM]_NODECPUS
#
#      [REAL|WRFM]_NODECPUS is a list of machines, processors, and IP addresses that define
#      the parallel computing environment when running the WRF real and model programs.
#
#        REAL_NODECPUS - Defines the environment when running the WRF "real" program
#        WRFM_NODECPUS - Defines the environment when running the ARW core
#
#      [REAL|WRFM]_NODECPUS has the following format:
#
#        [REAL|WRFM]_NODECPUS = hostname1:np:IP,hostname2:np:IP,...,hostnameN:np:IP
#      or
#        [REAL|WRFM]_NODECPUS = np  or [REAL|WRFM]_NODECPUS = local:np
#
#      Where "hostname" is the hostname of the system or node, "np" is the number of processors
#      to use on that node, and "IP" is the IP address to use for communication to the other
#      nodes.
#
#      The hostname, number of processors, and IP address and separated by a colon (:) with
#      each system in the list separated by a comma (,).
#
#      Using a literal "local" refers to the local machine on which the UEMS is started.
#
#      In the absence of either an IP address or hostname, the UEMS will attempt to resolve
#      the missing value through an often erroneous and convoluted sequence of system calls.
#
#      Important: On MULTI-HOMED systems that communicate over 2 or more subdomains, it is
#      critical that the IP address be specified for the private/isolated network!
#
#      Using either [REAL|WRFM]_NODECPUS = np  or [REAL|WRFM]_NODECPUS = local:np will result
#      in UEMS running the model on the local system ONLY with the number of processors
#      specified by "np".  In fact, including "local" anywhere in the list of machines will
#      cause the UEMS to use ONLY the local system. All other hostnames are ignored.
#
#      Also note that there is no difference between listing processor individually
#      or by using the ":np", so that:
#
#          [REAL|WRFM]_NODECPUS = machine1,machine1,machine1,machine1
#
#      is the same as
#
#          [REAL|WRFM]_NODECPUS = machine1:4
#
#      NOTE: Failure to correctly specify the machine and number of CPUs will result in the
#            real program and model executable being run on the local host with the number
#            of CPUs defined by OMP_NUM_THREADS (SOCKETS * CORES) in the EMS.cshrc file.
#
#      EXAMPLES
#
#        1. Running the WRF model on 3 nodes (node1,node2, and node3) each with 2 physical
#           CPUs and 4 cores on each CPU (8 total virtual processors on each node). All
#           communication will be done on subnet 10.1.1.:
#
#             WRFM_NODECPUS = node1:8:10.1.1.1,node2:8:10.1.1.2,node3:8:10.1.1.3
#
#        2. Running the WRF real program on the local machine only with 1 CPU and 2 cores
#           or 2 CPUS with 1 core each
#
#             REAL_NODECPUS = 2 or REAL_NODECPUS = local:2 or REAL_NODECPUS = local
#
#
#      SUGGESTION/ADVICE
#
#        A. I have found that it is faster to run the WRF real program on the LOCAL machine
#           rather than distributing the load across multiple nodes. This is probably due to
#           the intensive IO is creating the initial and boundary condition files.
#
#           Also, when running "REAL", it is important that you NOT decompose your domain
#           such that there is fewer than 8 grid points per tile side. This requirement
#           increases to 10 if if are running a nested simulation. Violating this rule will
#           result in your simulation crashing and  you will never know that the cause of the
#           crash was created during the creation of the initial and boundary conditions;
#           unless, of course, you are reading this guidance.
#
#        B. So if you are running on a small cluster it may be advantageous to set REAL_NODECPUS
#           to the local (master) node with max CPUs and them use your entire computing
#           power for the model run (WRFM_NODECPUS).
#
#        C. If you are using DECOMP = 1 below, then make sure that the number of grid points
#           per CPU tile (YDIM/(Total number of CPUs)) is at least 8 for a single domain
#           simulation and 10 for a nested domain simulation; otherwise, there will not be
#           enough grid points within each tile for computations and your simulation will
#           go up in flames.
#
#           And flames is never good unless you're grill'n something.
#
#        D. The above requirement for the minimum number of grid points in each direction of
#           a tile is true regardless of the method of decomposition.  Just because you choose
#           DECOMP method 0 (model decides) doesn't mean that you will like the decision. The
#           algorithm in the WRF will auto-decompose the number of CPUs into an Nx by My mesh,
#           where M > = N and M-N is as close to zero as possible. So if you are using 24 CPUs
#           the internal decomposition will be 4 x 6 CPUs. This method reduces the risk of
#           violating the requirement for the minimum number of grid points in each direction.
#
#           Notice that if you use a large prime number the chance that you will violate this
#           law of the computational land increases, so know what you are doing before you do
#           something stupid. And that goes for modeling too!
#
#
#        E. Important fatherly advice from your surrogate simulation daddy:
#
#           You are likely to see an improvement in performance by NOT using all the available
#           processors on the host machine. For example, if you have a 2x6 core machine (12 total
#           total processors), you may see a significant performance boost by using 11 of the 12
#           available cores.
#
#           That's correct, FEWER IS SOMETIMES BETTER!
#
#           As a test, ARW benchmark cases were run (12 hours) on a machine with a 2x6 core
#           configuration.  The results are an average of 3 consecutive simulations using
#           the benchtest utility (strc/Ebin/benchtest):
#
#             Benchmark Case            12 CPUs             11 CPUs      Performance Gain
#             --------------         -------------        ------------   ----------------
#
#               27april2011           311 Seconds         145 Seconds         114%
#
#           The reason for this is not completely clear. It may be the system kernel needs a
#           thread for system activities, which means it's constantly sharing a thread on
#           one of the CPUs with the model when 12 CPUs are used. Another possibility is that
#           WRF dedicates 1 thread to I|O leaving 11 processors for the 12 requested.
#
#        F. If Hyper-Threading is turned ON:
#
#           a.  Turn it OFF in the BIOS
#
#           b.  When hyper-threading is ON, the kernel sees twice as many cores (CPUs) on
#
REAL_NODECPUS = local:10
WRFM_NODECPUS = local:10


#  OPTION:  DECOMP (Possible values 0, 1, or 2)
#
#      DECOMP tells UEMS which method of domain decomposition to use when running on a parallel
#      distributed memory system. If you set DECOMP to 0, you are requesting that the WRF model
#      do an internal decomposition of your domain across the number of processors being used.
#      This value is equivalent to setting NPROC_X and NPROC_Y = -1 in the WRF namelist.
#
#      Setting DECOMP to 1 will decompose your domain in to 1 x N patches where N is the
#      number of processors being used. So if you are running on on 8 processors, then your
#      domain will be broken out as 1x8:
#
#          ---------------------------
#          |           8             | Patch Process 7 on Node 1
#          ---------------------------
#          |           7             | Patch Process 6 on Node 1
#          ---------------------------
#          |           6             | Patch Process 5 on Node 1
#          ---------------------------
#          |           5             | Patch Process 4 on Node 1
#          --------------------------- <--- Only communication interface between Nodes
#          |           4             | Patch Process 3 on Node 0
#          ---------------------------
#          |           3             | Patch Process 2 on Node 0
#          ---------------------------
#          |           2             | Patch Process 1 on Node 0
#          ---------------------------
#          |           1             | Patch Process 0 on Node 0
#          ---------------------------
#
#     This decomposition method should result in improved performance over the WRF internal
#     decomposition, that is until the internal decomposition improves.
#
#     From the WRF users guide: "The ARW core requires that no patch (subdomain allocated to
#     an MPI task) be no smaller in its horizontal dimensions than 6x6 for coarse domain and
#     no smaller than 9x9 for a nest. For each dimension of each of your domains, divide the
#     domain width by the number of tasks over which that domain will be decomposed. The result
#     should not be less than 6 for the coarse domain and not less than 9 for any nest. For
#     example, if you are running a simulation with a nested domain that is 424 in X and 325
#     in Y, then the largest number of tasks in X is 47 and in Y is 36 (for a total of 1692).
#     The smallest number in X and Y over all domains is the most tasks you can run for the
#     simulation."
#
#     That's the guidance from the WRF developers, which is good enough for the UEMS. Thus,
#     the UEMS has internal consistency checks just to make sure that you are doing your
#     due diligence. Should you fail then the UEMS will automatically revert to the WRF
#     decomposition routine (Option 0).
#
#     If you set DECOMP = 2, you are telling the UEMS that you want to manually define the
#     decomposition of the computational domain through the values specified by the
#     DECOMP_X and DECOMP_Y parameters.  It will be completely up to you to use values
#     that will not result in your run crashing. Be sure to read the quote from the WRF
#     user's guide above before using this option.
#
#     If DECOMP = 2, the UEMS will use the values of DECOMP_X and DECOMP_Y for the WRF
#     namelist variables NPROC_X and NPROC_Y, respectively. If DECOMP_X and _Y are not
#     specified, or the values are unreasonable, the UEMS will use DECOMP = 0.
#
#     Finally, when DECOMP = 2, the value of DECOMP_X * DECOMP_Y must be equal to the
#     number of CPUs specified above for WRFM; otherwise, DECOMP = 1 will be used.
#
#     Remember: DECOMP, DECOMP_X, and DECOMP_Y are only used for the decomposition of
#               the domain during the WRF simulation (WRFM).

#
#  DEFAULTS: DECOMP = 0
#            DECOMP_X = (blank)
#            DECOMP_Y = (blank)
#
DECOMP = 1
DECOMP_X = 
DECOMP_Y = 


#  OPTION:  NUMTILES
#
#    You can further improve the performance of your system by setting NUMTILES to a value
#    greater than 1. The NUMTILES setting will further subdivide the decomposed domain
#    patches into smaller tiles that are processed individually. The goal is to define
#    NUMTILES such that the amount of memory used for each tile can fit into the CPUs cache.
#
#    A NUMTILES value that is too large, thus making the size of the tiles (and memory
#    required) too small, will result in a degradation in performance or possible model
#    crash (seg fault). If the NUMTILES value is too small then you will may see a
#    reduced performance benefit.
#
#    Since "perfect" value for NUMTILES depends upon the type of CPU, the number of total
#    cores used in the simulation, and the size of your domain, it is impossible for the
#    Wizards of the UEMS to determine an algorithm for setting NUMTILES automatically. So it
#    is up to you to determine a value by trial and error. There is some general guidance,
#    because the UEMS does not want you wasting your time:
#
#    *  If you attempt to use the GFDL radiation scheme with the ARW core then NUMTILES
#       must be set to 1 since the GFDL scheme was designed to work with the NMM core,
#       which is now living on the "farm".
#
#    *  The number of NUMTILES specified should DECREASE with an INCREASE in the number of
#       PHYSICAL processors (or sockets) being used. This is because the size of the
#       patches are already getting smaller, and thus the memory used by each patch, when
#       increasing the number of CPUs.
#
#    *  On a single workstation with 2 CPUs and a total of 8 cores (4 each), start
#       with NUMTILES = 8 and then increase or decrease as necessary until you've
#       found the "sweet spot" for your domain. You may actually use any integer values.
#       greater than 0.
#
#    *  Expect to get a performance gain of ~6%, more if you are lucky, which you are,
#       simply by virtue of having the UEMS.
#
#    *  Should you get an error similar to:
#
#         *** glibc detected *** ..... double free or corruption (!prev): 0x00...
#
#       It likely means that you have over-decomposed the domain patches by requesting too
#       many tiles. In this case lower the NUMTILES value.
#
#    The default value is NUMTILES = 1 and has always been the default; however, it is almost
#    certain that you will see a performance gain by increasing this value.
#
NUMTILES = 1



#  OPTION:  MPICHECK
#
#    Setting MPICHECK to any value will cause the UEMS to run a simple network check
#    prior to running a simulation across multiple system. It's purpose is to ensure
#    that the machines listed in REAL_NODECPUS and WRFM_NODECPUS are reachable and
#    configured for running the UEMS. If you ran the "netcheck" utility previously
#    and feel comfortable that your cluster is configured correctly, then leave
#    leave MPICHECK blank and save yourself 30 seconds of execution time.
#
#  DEFAULT: MPICHECK = 1
#
MPICHECK = 1


#  OPTION:  HYDRA_IFACE - The network interface to use for communication
#
#    The UEMS uses the MPICH2 "HYDRA" processes manager for distributed computing.
#    The HYDRA_IFACE parameter tells HYDRA which network interface to use for the
#    communication between nodes. Available interfaces may be found by running the
#    "ifconfig" utility on your system where the interface names (eth0, lo, etc)
#    are listed on the left hand side of the output.
#
#    The default for HYDRA_IFACE is to leave it blank, in which case the UEMS
#    will attempt to determine the interface to use from the IP addresses of the
#    hosts specified in REAL_NODECPUS and WRFM_NODECPUS.
#
#    If the UEMS is run on the local host only, then leave HYDRA_IFACE blank as
#    it will not be ignored anyway.
#
#  DEFAULT:  HYDRA_IFACE = <blank>
#
HYDRA_IFACE = 


# ==============================================================================================
# NOTHING ELSE TO SEE HERE IN THE UEMS COMPUTATIONAL ENVIRONMENT CONFIGURATION FILE
# ==============================================================================================
